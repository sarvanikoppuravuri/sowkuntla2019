def IND(objects :Int ,row :Array[org.apache.spark.sql.Row] ) : List[List[Any]] ={
	var count=row.length
	var arr:List[List[Any]]=List(List())
	for(i<- 0 to count-1){
    	var y=arr(0):+row(i)(1)
    	arr=arr.updated(0,y)
	}
	var y=arr(0):+1
	arr=arr.updated(0,y)
	var z:List[Any]=List();
	for(i<-2 to objects){
		val length=arr.length;
		var ans="false";
		for(j<-0 to length-1){
			var x=0;
			for(k<-0 to count-1){
				if(arr(j)(k)==row(k)(i)){
					x=x+1;
				}
			};
			if(x==count){
				var y=arr(j):+i;
				arr=arr.updated(j,y);
				ans="true";
			}
		};
		if(ans=="false"){
			arr=arr:+List();
			for(l<- 0 to count-1){
				z=arr(length):+row(l)(i);
				arr=arr.updated(length,z);
			};
			z=arr(length):+i;
			arr=arr.updated(length,z); 
		}
	};
	return arr;
}

def compute_POS(count : Int,arr:List[List[Any]],decision_arr:List[List[Any]]  ) :  List[Any]={
	val d_count=decision_arr.length;
	var res:List[List[Any]]=List();
	for(i<-0 to d_count-1){
		res=res:+List();
		val c_count=arr.length;
		for(j<-0 to c_count-1){
			val array=arr(j).take(0) ++ arr(j).drop(0+count);
			val d=decision_arr(i).take(0) ++ decision_arr(i).drop(0+1);
			val intersect=d.intersect(array);
			if(intersect==array){
				for(l<-0 to intersect.length-1){
					var x=res(i):+intersect(l);
					res=res.updated(i,x);
				}
			}
		}
	}
	println("Resultant sets are"+res);
	val lowerapprox=res.flatten;
	println("Lower Approximation : "+lowerapprox);
	return lowerapprox;
}

def Local_Best(reduct:List[Any],decision_arr:List[List[Any]])(numbers : Iterator[List[Any]]) : Iterator[List[String]] ={
	var best_atrid="";
	var best_pos=0;
	while(numbers.hasNext){
		var b:List[Any]=numbers.next().toList
		var pos_region:List[Any]=List();
		val atr_temp:List[Any]=List(b(0));
		val red_intersect=atr_temp.intersect(reduct);
		if(red_intersect==List()){
			var atr_arr:List[List[Any]]=List(List(b(1),1));
			for(i<-2 to (b.length)-1){
				val length=atr_arr.length;
				var ans="false";
				for(j<-0 to length-1){
					if(atr_arr(j)(0)==b(i)){
						var y=atr_arr(j):+i;
						atr_arr=atr_arr.updated(j,y);
						ans="true";
					}
				};
				if(ans=="false"){
					atr_arr=atr_arr:+List((b(i)),i);
				}
			}
			var lower_approx=compute_POS(1,atr_arr,decision_arr);
			var pos=lower_approx.length.toInt;
			if(pos>=best_pos){
				best_pos=pos;
				best_atrid=b(0).toString;
				pos_region=lower_approx;
			}
		}
	}
	println("Local Best Attribute Id : "+best_atrid);
	println("Local Best POS count : "+best_pos.toString);
	return Iterator(List(best_atrid,best_pos.toString))
}

def Global_Best(local : Array[List[String]]) : List[String] ={
	var global_bestid="";
	var global_poscount=0;
	for(i<-0 to local.length-1){
		if(local(i)(1).toInt>global_poscount){
			global_poscount=local(i)(1).toInt;
			global_bestid=local(i)(0);
		}
	}
	return List(global_bestid,global_poscount.toString)
}

def Positive_Granule(id : String,row:Array[org.apache.spark.sql.Row],decision_arr:List[List[Any]]  ) : List[Any] ={
	var list:List[List[Any]]=List()
	for(i<-0 to row.length-1){
		list=list:+List();
		for(j<-0 to row(i).length-1){
			var x=list(i):+row(i)(j);
			list=list.updated(i,x);
		}
	}
	var index=0;
	for(k<-0 to list.length-1){
		if(list(k)(0)==id){
			index=k;
		}
	}
	var atr_arr:List[List[Any]]=List(List(list(index)(1),1));
	for(i<-2 to (list(index).length)-1){
		val length=atr_arr.length;
		var ans="false";
		for(j<-0 to length-1){
			if(atr_arr(j)(0)==list(index)(i)){
				var y=atr_arr(j):+i;
				atr_arr=atr_arr.updated(j,y);
				ans="true";
			}
		};
		if(ans=="false"){
			atr_arr=atr_arr:+List((list(index)(i)),i);
		}
	}	
	var pos=compute_POS(1,atr_arr,decision_arr)
	return pos
}

val csv=spark.read.format("csv").option("header","true").load("/table.csv")
csv.createOrReplaceTempView("csvTable")
spark.sqlContext.sql("select * from csvTable").show()
val decision=spark.read.format("csv").option("header","false").load("/decision.csv")
decision.createOrReplaceTempView("dTable")
spark.sqlContext.sql("select * from dTable").show()

var reduct:List[Any]=List();
println(reduct)
val objects=10;
val count1=csv.count()
val ca=count1.toInt;
var NP_GR = List.range(1, objects+1)

val row = csv.rdd.take(ca);
row;
var arr=IND(objects,row);
arr;
val decision_row = decision.rdd.take(1)
decision_row;
var decision_arr=IND(objects,decision_row);
decision_arr;

val count1=csv.count();
val ca=count1.toInt;
var lower_approx=compute_POS(ca,arr,decision_arr);
println("lower approx : "+lower_approx);
val pos_length=lower_approx.length;
var gamma_c=pos_length.toFloat/objects;
println("gamma_c is "+gamma_c);

var total_poscount=0;
var gamma_red=0.0f;
while(gamma_c!=gamma_red){
    var atr_row=csv.rdd.take(ca);
    var pos_count:List[Any]=List();
    var best_atrid=0;
    var best_pos=0;
    var pos_region:List[Any]=List();  
    var df=csv.rdd.take(csv.count().toInt)
    var arg:List[List[Any]]=List();
    for(i<-0 to df.length-1){
    	arg=arg:+List(df(i)(0));
    	for(j<-1 to df(i).length-1){
    		var x=arg(i):+df(i)(j);
     		arg=arg.updated(i,x);
    	}
    }
    var data = sc.parallelize(arg, 2)
    var lb=data.mapPartitions(Local_Best(reduct,decision_arr));
    var localbest=lb.collect
    var globalbest=Global_Best(localbest)
    var global_atrid=globalbest(0);
    var global_poscount=globalbest(1).toInt;
    println("Global Best Attribute id : "+global_atrid);
    println("Global Best POS count : "+global_poscount);
 	var posregion=Positive_Granule(global_atrid,row,decision_arr);
    reduct=reduct:+global_atrid;
    total_poscount+=global_poscount;
    gamma_red=(total_poscount).toFloat/objects;
    println("Reduct is : "+reduct);
    println("Gamma Reduct is " +gamma_red);
    if(gamma_red<gamma_c){
		NP_GR= NP_GR diff posregion;
		println("Non postive Region : "+NP_GR);
	}
}
println("Non positive Granule : "+NP_GR);
println("Final Reduct  : "+reduct);
